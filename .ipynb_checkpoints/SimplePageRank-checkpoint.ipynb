{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank Algorithm on Spark \n",
    "\n",
    "Hi, I'm a beginner on Spark, coding in Python. I was looking for a very simple first project and read multiple explanations of the PageRank algorithm but none that didn't require using the Command Prompt. I hope my attempt, with a lot of explanations, is helpful.\n",
    "\n",
    "## I The PageRank Theory\n",
    "\n",
    "Equations are directed graphs below are based on \"Data-Intensive Text Processing with MapReduce\" by Jimmy Lin and Chris Dyer.  \n",
    "The PageRank of a page is defined as the probability that the random websurfer will view page p:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Rank(p)=\\alpha \\frac{1}{GraphNodes} +(1-\\alpha)\\left(\\frac{1}{GraphNodes}\\sum_{d \\in DanglingNodes}Rank(d)+\\sum_{k \\in InLinks(p)}\\frac{Rank(k)}{OutLinks(k)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $\\alpha$ the random jump factor (probability that the random websurfer visits any page) and $1-\\alpha$ the probability he follows a link.\n",
    "\n",
    "The probability that the random websurfer will arrive from k in the Inlinks of page p to p is $\\frac{Rank(k)}{OutLinks(k)}$ and we sum this probability over all inlinks.\n",
    "\n",
    "![PageRank Graph Nodes](Graph.png)\n",
    "\n",
    "In the Map step, probability mass is distributed along the vertices.  \n",
    "In the Shuffle and Sort step, probabilities are grouped by destination node.   \n",
    "In the Reduce step, probabilities from incoming links are summed to update the node's ranking.  \n",
    "\n",
    "\n",
    "Initially we set $\\alpha$ to 0 and implement the directed graph of the book with no dangling nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II PageRank Python Code Multithreading on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyquickhelper.filehelper import remove_folder\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "import timeit\n",
    "\n",
    "sc = SparkContext(appName=\"PythonPageRank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is where a directed graph is created, initially to resemble that of the book.\n",
    "Other inputs control how many iterations we will do, whether we stop at convergence or impose a limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Section for user inputs\n",
    "\n",
    "#Build a directed graph which simulates the result of a webcrawler.\n",
    "L1='l1'\n",
    "L2='l2'\n",
    "L3='l3'\n",
    "L4='l4'\n",
    "L5='l5'\n",
    "\n",
    "alpha=0 #random jump factor\n",
    "maxIter=10 # regardless of convergence will stop at set number of iterations. Beware, 6 iterations take 6 min!\n",
    "tolerance=0.01 # will stop if successive rank results are within a certain tolerance of each other (convergence).\n",
    "gchoice='book' # choice of directed graph to test PageRank on: book, dangle or repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to group the computation of dangling node mass and rank mass propagation into one referral function but the assignment to the global variable didn't work in the lambda function so we split into 2 distict functions instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Section for function definitions\n",
    "\n",
    "def chooseGraph(gtype):\n",
    "    ''' User selects the type of directed graph to test PageRank on. Choices are 'book', 'dangle' or 'repeat. Returns the graph as a tuple. '''\n",
    "    if gtype=='book':\n",
    "        graph = ((L1,(L2,L4)), (L2,(L3,L5)), (L3,(L4,)), (L4,(L5,)), (L5,(L1,L2,L3))) # as in book\n",
    "    elif gtype=='dangle':\n",
    "        graph = ((L1,(L2,L4)), (L2,(L3,L5)), (L3,(L4,)), (L4,()), (L5,(L1,L2,L3))) # L4 is dangling\n",
    "    elif gtype=='repeat':\n",
    "        # L5 key repeated (the webcrawler could crawl a page twice by mistake.) \n",
    "        # The case where a key has different values is purposely not considered. \n",
    "        # A crawler can't get different results on the same page.\n",
    "        graph = ((L1,(L2,L4)), (L2,(L3,L5)), (L3,(L4,)), (L4,(L5,)), (L5,(L1,L2,L3)),(L5,(L1,L2,L3))) \n",
    "    else:\n",
    "        print('Error: Incorrect choice of Graph')\n",
    "    return graph\n",
    "            \n",
    "def referral(urls, rank):\n",
    "    '''Calculates URL referral weight to the rank of the URLs it references.'''\n",
    "    urlsNb = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / urlsNb) #this returns a generator (only once iterable), lazy execution\n",
    "        \n",
    "def lostMass(urls,rank):\n",
    "    '''Calculates lost mass for a dangling node.'''\n",
    "    urlsNb = len(urls)\n",
    "    if urlsNb==0:\n",
    "        global pmdn\n",
    "        pmdn=pmdn+rank #probability mass of dangling node is passed on\n",
    "        yield pmdn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initiate the parallelization process in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Section for initializing the process in Spark\n",
    "        \n",
    "DirGraph=chooseGraph(gchoice)\n",
    "nodenb=len(DirGraph)\n",
    "\n",
    "nodesRDD = sc.parallelize(DirGraph, nodenb) #splits the problem over as many CPUs as there are nodes\n",
    "\n",
    "# A webcrawler might get the same link mutliple times as key, so we use distinct\n",
    "nodesRDD = nodesRDD.distinct()\n",
    "#nodesRDD.collect() #check\n",
    "\n",
    "# The same Graph is iterated over multiple times. To improve efficiency we cache the data in memory.\n",
    "# We use persist() as opposed to cache() so that the level of memory storing is made explicit.\n",
    "nodesRDD = nodesRDD.persist(pyspark.StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we implement the pagerank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pagerank algorithm performed the maximum value of 1 iterations in 45.3 seconds without reaching a level of convergence between successive page rank results of 0.01. Page ranks are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('l3', 0.2066666666666667),\n",
       " ('l2', 0.2066666666666667),\n",
       " ('l5', 0.14),\n",
       " ('l1', 0.10666666666666666),\n",
       " ('l4', 0.34)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section for PageRank algorithm\n",
    "\n",
    "startTime = timeit.default_timer() #check algo speed\n",
    "\n",
    "# Initializes all nodes to a weight of 1/nodenb which is the probability that they are chosen at random.      \n",
    "ranks = nodesRDD.map(lambda outlinks: (outlinks[0], 1/nodenb))\n",
    "#ranks.collect()\n",
    "ranksmem=ranks.lookup(L1)[0] #initialize rank memory for node L1 to check for convergence\n",
    "\n",
    "for k in range(maxIter): # Calculates and updates URL ranks iteratively\n",
    "\n",
    "    countIter=k+1 # tracks iterations to reach convergence if \"tolerance\" parameter breaks the loop\n",
    "    pmdn=0 # probability mass of dangling nodes\n",
    "    ranks = nodesRDD.join(ranks)\n",
    "    \n",
    "    # calculates dangling nodes' lost mass. genitem:(node,((outlink1, outlink2),rank)).\n",
    "    pmdn= ranks.flatMap(lambda genitem: lostMass(genitem[1][0], genitem[1][1])).sum()\n",
    "    # Calculates URL contributions to rank of other URLs. \n",
    "    ranks= ranks.flatMap(lambda genitem: referral(genitem[1][0], genitem[1][1]))\n",
    "\n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    # We use add as opposed to \"lambda x, y: x+y\" because we read it is faster\n",
    "    # groupbykey is less efficient (more shuffling of data between CPU) with large databases\n",
    "    ranks = ranks.reduceByKey(add)\n",
    "    ranks = ranks.mapValues(lambda rank: alpha*(1/nodenb) + (1-alpha)*(pmdn/nodenb+rank))\n",
    "    \n",
    "    # the line below may raise exception: \"Randomness of hash of string should be disabled via PYTHONHASHSEED\"\n",
    "    # solution: edit \"path\" in Windows Environment Variables with a new variable PYTHONHASHSEED and set the value to 0.\n",
    "    ranksnew=ranks.lookup(L1)[0] \n",
    "    if abs(ranksnew-ranksmem)<abs(tolerance): #in case user sets a negative tolerance\n",
    "        break\n",
    "    ranksmem=ranksnew\n",
    "   \n",
    "finalRanks=ranks.collect()\n",
    "algoTime = timeit.default_timer() - startTime\n",
    "\n",
    "# Displaying and results\n",
    "\n",
    "if countIter==maxIter:\n",
    "    print(\"The pagerank algorithm performed the maximum value of {} iterations in {:0.1f} seconds without reaching a level of convergence between successive page rank results of {}. Page ranks are:\".format(maxIter, algoTime, tolerance))\n",
    "else:\n",
    "    print('With {} iterations in {:0.1f} seconds page rank results have converged with successive results within {} of each other. Page ranks are:'.format(countIter, algoTime, tolerance))\n",
    "\n",
    "finalRanks #display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results found match those in the book at the first and second iteration. For the first iteration (gchoice='book', alpha=0, maxIter=1) you should get:\n",
    "![PageRank Graph Nodes](Book1.png)\n",
    "\n",
    "Convergence at the 0.01 tolerance level is reached at the 6th iteration. What is remarkable is that the pagerank algorithm converges with so few iterations and also that Spark requires so much time per iteration.\n",
    "![PageRank Speed](Speed.png)\n",
    "\n",
    "Granted the time to map/reduce is predominent for small directed graphs and that fixed cost becomes negligible for large graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may repeat the algorithm testing with one dangling node (gchoice='dangle', alpha=0, maxIter=1) and verify manually that the first iteration results in the appropriate ranks.\n",
    "![PageRank Speed](Dangle1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this helped!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
